{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f9eb642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "938487d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return(np.exp(x)/np.exp(x).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab314c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(x, num_heads):\n",
    "  split_head = np.reshape(x, (np.shape(x)[0], np.shape(x)[1], num_heads, -1))\n",
    "  return split_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a81b3d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value):\n",
    "\n",
    "    # Compute dot product of query and key\n",
    "    dot_product=np.matmul(query, np.transpose(key, (0,2,1)))  \n",
    "    #print(\"dot product shape:\", dot_product.shape)\n",
    "\n",
    "    # Calculating Scale\n",
    "    key_dim=key.shape[-1]\n",
    "    #print(key_dim)\n",
    "    \n",
    "    # Scale dot-product by key dimension\n",
    "    scaled_dot_product = dot_product / np.sqrt(key_dim)\n",
    "    #print(\"scaled dot product shape:\", scaled_dot_product.shape)\n",
    "\n",
    "    # Compute attention weights by softmax application\n",
    "    attention_weights=softmax(scaled_dot_product)\n",
    "    #print(\"attention weights:\", attention_weights.shape)\n",
    "\n",
    "    # Compute attention output\n",
    "    scaled_dot_product_attention_output=np.matmul(attention_weights, value)\n",
    "    #print(\"attention output:\", attention_output.shape)\n",
    "\n",
    "    return scaled_dot_product_attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dd7bb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_scaled_attention(query, key, value, num_heads, W_q, W_k, W_v):\n",
    "  \n",
    "    # Get the linearly projected values of query, key and value\n",
    "    \n",
    "    query = np.matmul(query,W_q)\n",
    "    key = np.matmul(key,W_k)\n",
    "    value = np.matmul(value, W_v)\n",
    "    \n",
    "    # Split q, k, and v into num_heads separate 2D tensors\n",
    "    \n",
    "    #queries = np.reshape(query, (np.shape(query)[0], np.shape(query)[1], num_heads, -1))\n",
    "    queries = split_heads(query, num_heads)\n",
    "    #print(\"Queries init: = \",queries.shape)\n",
    "\n",
    "    #keys = np.reshape(key, (np.shape(key)[0], np.shape(key)[1], num_heads, -1))\n",
    "    keys = split_heads(key, num_heads)\n",
    "    #print(\"Keys init: =\", keys.shape)\n",
    "\n",
    "    #values = np.reshape(value, (np.shape(value)[0], np.shape(value)[1], num_heads, -1))\n",
    "    values = split_heads(value, num_heads)\n",
    "    #print(\"Values init: =\",values.shape)\n",
    "\n",
    "    # Concatenate attentions by calculating the attention for each head\n",
    "    attention_outputs = []\n",
    "\n",
    "    for i in range(num_heads):\n",
    "        attention_output=scaled_dot_product_attention(queries[:,:,:,i], keys[:,:,:,i], values[:,:,:,i])\n",
    "        attention_outputs.append(attention_output)\n",
    "\n",
    "    # Make an array of concatenated attention outputs\n",
    "    attention = np.array(attention_outputs)\n",
    "    #print(\"Concat attention:\", attention.shape)\n",
    "\n",
    "    # Project the concatenated attention back to the original size\n",
    "    multi_head_attention_output = np.transpose(attention, (1, 2, 3, 0))\n",
    "    multi_head_attention_output = np.reshape(multi_head_attention_output, (multi_head_attention_output.shape[0], multi_head_attention_output.shape[1], -1))\n",
    "    #print(\"Multi Head Attention Output:\", multi_head_attention_output.shape)\n",
    "\n",
    "    return multi_head_attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a106b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing out with following input values \n",
    "\n",
    "input_seq_len=5 # Maximum length of the input sequence\n",
    "d_q=64          # Dimensionality of thge linearly projected queries\n",
    "d_k=64          # Dimensionality of the linearly projected keys\n",
    "d_v=64          # Dimensionality of the linearly projected values\n",
    "batch_size=64   # Batch size from the training process\n",
    "num_heads=8     # Number of self-attention heads\n",
    "\n",
    "query = np.random.randn(batch_size, input_seq_len, d_q)   # generating input query matrix\n",
    "key = np.random.randn(batch_size, input_seq_len, d_k)     # generating input key matrix\n",
    "value = np.random.randn(batch_size, input_seq_len, d_v)   # generating input value matrix\n",
    "\n",
    "W_q = np.random.randn(d_q, d_q)                           # for generating num head projection matrices for queries\n",
    "W_k = np.random.randn(d_k, d_k)                           # for generating num head projection matrices for keys\n",
    "W_v = np.random.randn(d_v, d_v)                           # for generating num head projection matrices for values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10d2d22a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Dot Product Attention: [[[-5.42331165e-05  8.93264585e-04 -3.62929491e-04 ... -1.53482501e-03\n",
      "   -1.64819457e-03  4.49659776e-04]\n",
      "  [-3.51388413e-04 -9.18896634e-05  2.43727082e-05 ... -7.87239600e-04\n",
      "   -6.95673509e-04  1.87105840e-03]\n",
      "  [-3.80599868e-04  1.25855370e-03  7.43379396e-05 ... -2.57113688e-03\n",
      "   -2.94539998e-03  4.09979698e-03]\n",
      "  [-2.10901490e-04  5.92957119e-04 -8.53901711e-04 ... -2.07863604e-03\n",
      "   -2.17466432e-03  1.17826968e-03]\n",
      "  [-2.93210528e-06  5.43493552e-04  1.66037258e-04 ... -1.42462097e-03\n",
      "   -1.44958928e-03  1.67015369e-03]]\n",
      "\n",
      " [[-1.38830841e-03  4.85514576e-03  1.64503101e-03 ...  2.32063376e-03\n",
      "   -1.13489613e-03 -2.03268644e-03]\n",
      "  [-7.09308490e-04  1.22612488e-03 -2.06180656e-04 ...  6.22827005e-04\n",
      "   -4.06224227e-04 -3.49757754e-04]\n",
      "  [-2.22514445e-03  1.14259259e-02  4.14821600e-03 ...  4.95919281e-03\n",
      "   -1.53045199e-03 -4.93872052e-03]\n",
      "  [-7.59862671e-04  1.91651466e-03 -1.39841116e-04 ...  5.29781486e-04\n",
      "   -1.91300142e-04 -4.11299925e-04]\n",
      "  [-3.46314013e-04  1.57737940e-03  3.85141924e-04 ...  5.54859837e-04\n",
      "   -8.35845390e-05 -5.58176707e-04]]\n",
      "\n",
      " [[ 8.55181175e-08  1.32711438e-03  8.09918126e-04 ... -2.50212342e-04\n",
      "   -4.36665268e-04  1.58843025e-03]\n",
      "  [-2.01340282e-03  1.28357456e-03 -1.27408894e-05 ... -1.53927466e-03\n",
      "   -9.01545544e-04  1.27009149e-03]\n",
      "  [-1.43736829e-03  1.10339435e-03  1.77377894e-03 ... -7.96194652e-04\n",
      "   -1.96963187e-03  1.20841410e-03]\n",
      "  [-5.07364453e-04  1.68379156e-03  1.43866839e-03 ... -6.17963152e-04\n",
      "   -1.11892292e-03  1.91698599e-03]\n",
      "  [ 2.42336824e-03  3.78009246e-03  3.47789849e-03 ... -2.96732888e-03\n",
      "    6.60110439e-04  2.97381315e-03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-2.39187230e-04 -2.34542469e-04 -2.39988502e-04 ... -8.70580618e-04\n",
      "    4.29386908e-04 -4.26272344e-04]\n",
      "  [-8.61900073e-04 -2.46420478e-04  3.41014006e-05 ... -1.90618897e-03\n",
      "    9.51947017e-04 -3.85667469e-04]\n",
      "  [-2.08517651e-03  2.15074185e-04  1.15557543e-03 ... -6.14178944e-03\n",
      "    1.57950012e-03  8.08001985e-04]\n",
      "  [ 9.10349674e-04  1.20941256e-03  2.34723101e-04 ... -2.97501081e-03\n",
      "    1.79186782e-03 -1.49231670e-03]\n",
      "  [ 5.62871495e-04 -2.04315988e-05 -1.13205008e-03 ... -3.76243455e-03\n",
      "    1.21925729e-03 -1.70272629e-03]]\n",
      "\n",
      " [[-2.17391622e-04  6.43827889e-06 -2.13884674e-04 ...  3.43835908e-04\n",
      "    5.24952760e-04  6.31553303e-04]\n",
      "  [ 2.90245825e-04  3.82367593e-04 -8.99836029e-04 ...  7.73968404e-04\n",
      "    8.64351227e-04  8.80902248e-04]\n",
      "  [ 3.29499357e-03 -2.34089387e-03 -3.66616145e-03 ...  3.98273223e-03\n",
      "   -2.22297918e-03 -1.58287779e-03]\n",
      "  [-5.20837150e-04 -2.15592785e-04  2.57791726e-04 ...  7.37592321e-04\n",
      "    3.32957113e-04  1.21290133e-03]\n",
      "  [-6.04098033e-04  4.62678057e-05  3.31711314e-04 ... -3.09906797e-05\n",
      "    5.13000932e-04  9.01902603e-04]]\n",
      "\n",
      " [[ 4.43484634e-04  3.42428349e-03 -4.28454011e-04 ... -1.48464327e-03\n",
      "   -1.62775837e-04  3.16676629e-03]\n",
      "  [-4.48951271e-04  1.49543313e-03 -1.20383662e-03 ... -1.80220620e-03\n",
      "    1.40539780e-04  6.43798673e-04]\n",
      "  [-4.77965732e-04  1.46118842e-03 -1.13108164e-03 ... -1.23302469e-03\n",
      "    5.61134261e-05  8.26333894e-04]\n",
      "  [ 5.77101252e-04  3.11865523e-03 -4.00568089e-04 ... -1.52653383e-03\n",
      "   -1.04393352e-04  2.96443286e-03]\n",
      "  [ 4.65196110e-04  1.78928185e-03 -5.69409831e-04 ... -6.11426974e-03\n",
      "    2.03101476e-04  2.42046119e-03]]]\n",
      "Scaled Dot Product Attention Shape: (64, 5, 64)\n"
     ]
    }
   ],
   "source": [
    "# Testing code of scaled dot product attention\n",
    "\n",
    "attention=scaled_dot_product_attention(query, key, value)\n",
    "print(\"Scaled Dot Product Attention:\", attention)\n",
    "print(\"Scaled Dot Product Attention Shape:\", attention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c5615f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi Head Scaled Attention [[[-6.82548377e-085  2.49936909e-132  1.69240724e-114 ...\n",
      "    1.16807424e-108 -7.62068133e-064  2.29188417e-075]\n",
      "  [-1.47432394e-132  7.39977915e-109  8.23297448e-091 ...\n",
      "    4.81963390e-125  2.77499089e-084 -3.12620788e-064]\n",
      "  [-2.73078037e-125  3.86699723e-124  1.00098713e-093 ...\n",
      "    2.18581769e-081 -7.47450150e-093 -8.90008576e-072]\n",
      "  [-3.75508114e-099 -8.62033077e-053  2.16251409e-126 ...\n",
      "    8.46616557e-126  1.27894956e-087  8.96634175e-067]\n",
      "  [-2.35275564e-067  6.17343298e-123  1.38601228e-086 ...\n",
      "   -6.03470506e-115  3.23234402e-102  7.77983719e-082]]\n",
      "\n",
      " [[ 2.18208946e-111  9.89663008e-093  3.44639193e-123 ...\n",
      "   -4.69289384e-114  2.41793168e-102  1.19141644e-061]\n",
      "  [-2.15476646e-096  8.36805203e-105  1.59630012e-117 ...\n",
      "   -3.17069982e-107  2.17700555e-101 -1.36687805e-074]\n",
      "  [ 7.00885194e-125  2.87290024e-128  1.24635476e-055 ...\n",
      "   -1.91052387e-112 -8.66578646e-106  2.64999271e-093]\n",
      "  [ 1.29097858e-097  7.13375964e-090  3.25965575e-097 ...\n",
      "   -2.98950859e-084 -6.50680472e-076  9.82754325e-055]\n",
      "  [ 7.54317949e-073  7.53418194e-120  8.41718975e-116 ...\n",
      "    7.48518650e-139 -1.77903075e-106 -2.14600474e-070]]\n",
      "\n",
      " [[-8.22218990e-113  1.13471910e-118  2.68551423e-070 ...\n",
      "    1.09808818e-135 -1.87148073e-088 -3.50844204e-058]\n",
      "  [ 9.02907985e-069  9.44158767e-080  2.98855105e-105 ...\n",
      "    1.90000157e-109  1.59549270e-104  1.15769447e-066]\n",
      "  [-1.08595166e-131  3.16246722e-117  8.09973144e-109 ...\n",
      "    7.19154439e-111 -5.56614185e-100 -1.89438090e-082]\n",
      "  [-3.01766441e-123 -1.52938364e-106 -1.48380705e-073 ...\n",
      "   -1.39226304e-114 -3.00330411e-114 -7.14683222e-059]\n",
      "  [ 7.85849944e-048  1.05126937e-099  1.37184064e-082 ...\n",
      "    2.93805068e-111 -1.30688974e-107  1.93646674e-024]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-9.71254790e-081  4.21442017e-153  4.02929531e-102 ...\n",
      "   -2.07610514e-084  1.46824289e-047  7.40118692e-060]\n",
      "  [-6.31148248e-115  1.33892992e-114  2.09681059e-126 ...\n",
      "   -8.15219277e-107  3.56537640e-092  1.75084480e-072]\n",
      "  [-6.87096674e-087  2.10388303e-110  2.54183134e-094 ...\n",
      "   -1.74666778e-093 -3.05382013e-103  6.77342090e-034]\n",
      "  [-4.04846006e-103 -5.67350450e-088 -1.94614058e-121 ...\n",
      "   -2.07894685e-127 -1.30714341e-064  1.14838989e-081]\n",
      "  [-1.59448913e-118  8.91675975e-136  9.62135730e-102 ...\n",
      "   -3.88818183e-069  2.42219279e-076  3.37934177e-057]]\n",
      "\n",
      " [[ 3.87736237e-100 -1.64999041e-124  1.91000226e-112 ...\n",
      "   -2.43608668e-083  1.27935012e-068 -4.23720218e-071]\n",
      "  [-1.89105839e-111  8.36056096e-104  6.01044659e-091 ...\n",
      "    4.59379049e-122  7.44964959e-061 -5.96387724e-098]\n",
      "  [ 1.14871004e-103  2.42535663e-114  1.16437938e-060 ...\n",
      "   -4.38518082e-072 -2.43178045e-032 -6.50705238e-070]\n",
      "  [ 2.97291945e-076  2.55487094e-139  2.13722511e-108 ...\n",
      "    3.80726450e-087  6.11224701e-106 -1.45185030e-045]\n",
      "  [ 2.23466027e-072 -2.94911491e-130  5.68104285e-065 ...\n",
      "    4.19109316e-106  2.45550619e-060 -9.21072428e-089]]\n",
      "\n",
      " [[ 7.98262125e-092  2.46976090e-116  8.12533788e-100 ...\n",
      "   -1.43210110e-113 -5.35785170e-096 -2.84262797e-089]\n",
      "  [ 3.92541551e-107 -3.89917476e-113 -7.63539104e-106 ...\n",
      "    5.74809581e-119 -1.27661706e-099 -7.41180744e-067]\n",
      "  [ 8.47179046e-116  2.82483287e-119  6.97506044e-071 ...\n",
      "   -7.03906018e-129 -3.04703105e-101  5.64937354e-085]\n",
      "  [ 2.31189704e-103  1.21500324e-082 -1.02601758e-096 ...\n",
      "   -5.32291223e-133 -6.59823828e-135 -1.03821533e-067]\n",
      "  [ 4.35028701e-090  1.76114566e-094 -4.64032124e-112 ...\n",
      "   -3.69196964e-131 -1.17598120e-077 -4.24085749e-077]]]\n",
      "Multi Head Scaled Attention Shape: (64, 5, 64)\n"
     ]
    }
   ],
   "source": [
    "# Testing code of multi head scaled attention\n",
    "\n",
    "multi_head_attention=multi_head_scaled_attention(query, key, value, num_heads, W_q, W_k, W_v)\n",
    "print(\"Multi Head Scaled Attention\", multi_head_attention)\n",
    "print(\"Multi Head Scaled Attention Shape:\", multi_head_attention.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
